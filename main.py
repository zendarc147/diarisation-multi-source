#!/usr/bin/env python3
"""
Diarisation multi-source pour interviews
Traite deux pistes audio sï¿½parï¿½es (prï¿½sentateur et invitï¿½)
"""

import os
import argparse
import torch
import torchaudio
from pyannote.audio import Pipeline
from datetime import timedelta


def format_timestamp(seconds):
    """Convertir les secondes en format HH:MM:SS.mmm"""
    return str(timedelta(seconds=seconds))


def process_audio_track(audio_path, pipeline, speaker_label):
    """
    Traiter une piste audio avec pyannote pour dï¿½tecter quand la personne parle

    Args:
        audio_path: Chemin vers le fichier audio
        pipeline: Pipeline pyannote.audio
        speaker_label: Label du locuteur (ex: "Prï¿½sentateur", "Invitï¿½")

    Returns:
        Liste de segments avec timestamps
    """
    print(f"\n<ï¿½ Traitement de {speaker_label}: {audio_path}")

    # Charger l'audio
    waveform, sample_rate = torchaudio.load(audio_path)

    # Appliquer la VAD (Voice Activity Detection)
    # Note: Vous aurez besoin d'un token HuggingFace pour utiliser les modï¿½les
    try:
        diarization = pipeline(audio_path)
    except Exception as e:
        print(f"ï¿½  Erreur lors de la diarisation: {e}")
        print("=ï¿½ Vous devez vous authentifier avec HuggingFace:")
        print("   1. Crï¿½ez un compte sur https://huggingface.co")
        print("   2. Acceptez les conditions d'utilisation des modï¿½les pyannote")
        print("   3. Gï¿½nï¿½rez un token d'accï¿½s")
        print("   4. Lancez: huggingface-cli login")
        return []

    # Extraire les segments de parole
    segments = []

    # Pyannote 4.0+ retourne un DiarizeOutput qui contient des Annotations
    try:
        # DiarizeOutput contient speaker_diarization (Annotation)
        if hasattr(diarization, 'speaker_diarization'):
            # Pyannote 4.0+ avec DiarizeOutput
            annotation = diarization.speaker_diarization
        elif hasattr(diarization, 'itertracks'):
            # Ancienne API, c'est dÃ©jÃ  un Annotation
            annotation = diarization
        else:
            # Essai de rÃ©cupÃ©ration gÃ©nÃ©rique
            annotation = diarization

        # ItÃ©rer sur les segments
        for turn, _, _ in annotation.itertracks(yield_label=True):
            segments.append({
                'start': turn.start,
                'end': turn.end,
                'duration': turn.end - turn.start,
                'speaker': speaker_label
            })

    except Exception as e:
        print(f"âš ï¸  Erreur lors de l'extraction des segments: {e}")
        print(f"    Type de l'objet retournÃ©: {type(diarization)}")
        print(f"    Attributs disponibles: {[a for a in dir(diarization) if not a.startswith('_')]}")
        return []

    return segments


def merge_segments(segments_presentateur, segments_invite):
    """
    Fusionner et trier les segments des deux locuteurs
    """
    all_segments = segments_presentateur + segments_invite
    all_segments.sort(key=lambda x: x['start'])
    return all_segments


def save_results(segments, output_path):
    """
    Sauvegarder les rï¿½sultats dans un fichier texte
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("DIARISATION - Rï¿½sultats\n")
        f.write("=" * 80 + "\n\n")

        for i, seg in enumerate(segments, 1):
            start_time = format_timestamp(seg['start'])
            end_time = format_timestamp(seg['end'])
            duration = format_timestamp(seg['duration'])

            f.write(f"Segment {i:03d}\n")
            f.write(f"  Locuteur: {seg['speaker']}\n")
            f.write(f"  Dï¿½but:    {start_time}\n")
            f.write(f"  Fin:      {end_time}\n")
            f.write(f"  Durï¿½e:    {duration}\n")
            f.write("-" * 80 + "\n")

    print(f"\n Rï¿½sultats sauvegardï¿½s dans: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Diarisation multi-source pour interviews"
    )
    parser.add_argument(
        "--presentateur",
        required=True,
        help="Chemin vers le fichier audio du prï¿½sentateur"
    )
    parser.add_argument(
        "--invite",
        required=True,
        help="Chemin vers le fichier audio de l'invitï¿½"
    )
    parser.add_argument(
        "--output",
        default="results/diarisation.txt",
        help="Chemin du fichier de sortie"
    )
    parser.add_argument(
        "--hf-token",
        help="Token HuggingFace (optionnel si dï¿½jï¿½ connectï¿½)"
    )

    args = parser.parse_args()

    # Vï¿½rifier que les fichiers existent
    if not os.path.exists(args.presentateur):
        print(f"L Erreur: Fichier non trouvï¿½: {args.presentateur}")
        return

    if not os.path.exists(args.invite):
        print(f"L Erreur: Fichier non trouvï¿½: {args.invite}")
        return

    print("=ï¿½ Dï¿½marrage de la diarisation multi-source")
    print(f"=ï¿½ Prï¿½sentateur: {args.presentateur}")
    print(f"=ï¿½ Invitï¿½: {args.invite}")

    # Charger le pipeline pyannote
    print("\n=ï¿½ Chargement du modï¿½le de diarisation...")
    try:
        if args.hf_token:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=args.hf_token
            )
        else:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1"
            )

        # Utiliser le GPU si disponible
        if torch.cuda.is_available():
            pipeline.to(torch.device("cuda"))
            print(" GPU dï¿½tectï¿½ et utilisï¿½")
        else:
            print("9 Utilisation du CPU")

    except Exception as e:
        print(f"L Erreur lors du chargement du modï¿½le: {e}")
        print("\n=ï¿½ Pour utiliser pyannote.audio, vous devez:")
        print("   1. Accepter les conditions sur: https://huggingface.co/pyannote/speaker-diarization-3.1")
        print("   2. Vous connecter: huggingface-cli login")
        return

    # Traiter les deux pistes
    segments_presentateur = process_audio_track(
        args.presentateur,
        pipeline,
        "Prï¿½sentateur"
    )

    segments_invite = process_audio_track(
        args.invite,
        pipeline,
        "Invitï¿½"
    )

    # Fusionner les rï¿½sultats
    print("\n= Fusion des segments...")
    all_segments = merge_segments(segments_presentateur, segments_invite)

    print(f"\n=ï¿½ Statistiques:")
    print(f"   Total segments: {len(all_segments)}")
    print(f"   Segments prï¿½sentateur: {len(segments_presentateur)}")
    print(f"   Segments invitï¿½: {len(segments_invite)}")

    # Sauvegarder les rï¿½sultats
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    save_results(all_segments, args.output)

    print("\n( Traitement terminï¿½!")


if __name__ == "__main__":
    main()
